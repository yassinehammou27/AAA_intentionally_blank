{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer: If you do \"Run All\", let everything load first, and dont interact with the maps in the clustering part without everything being loaded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import geoplot as gplt\n",
    "import contextily as cx\n",
    "\n",
    "# also import these \"new\" libraries \n",
    "# Note: you may have to download and add them to your environment (using e.g. 'conda install -c conda-forge folium')\n",
    "# !important! Install this version of plotly=5.10.0 or else some maps and animations may not render correctly\n",
    "import plotly.express as px\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "# import the necessary libraries for the machine learning models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from h3 import h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is set, to display all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load data\n",
    "taxi_df_clustering = pd.read_parquet('data/01_prep/prepared/taxi_data_prepared.gzip')\n",
    "df_h3 = pd.read_csv('data/03_clustering/hexagons.csv').drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will sample our data, since it will either take way too long to compute or crash the entire kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df_clustering = taxi_df_clustering.sample(200000, random_state=0)\n",
    "taxi_df_clustering.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Data Prep\n",
    "Before we can start with the clustering, we add addtional features to our clustering dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional information extracted from the timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional Data of weekday, hour and month\n",
    "taxi_df_clustering.loc[:,\"weekday\"] = taxi_df_clustering[\"trip_start_timestamp\"].dt.weekday\n",
    "taxi_df_clustering.loc[:,\"start_hour\"] = taxi_df_clustering[\"trip_start_timestamp\"].dt.hour\n",
    "taxi_df_clustering.loc[:,\"end_hour\"] = taxi_df_clustering[\"trip_end_timestamp\"].dt.hour\n",
    "taxi_df_clustering.loc[:,\"month\"] = taxi_df_clustering[\"trip_start_timestamp\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df_clustering = taxi_df_clustering.merge(df_h3[['pickup_centroid_location','h3_6']], how=\"left\", on=\"pickup_centroid_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_poi_for_resolution(resolution=\"h3_6\"):\n",
    "    # get polygons from hexagon id\n",
    "    df_hex = gpd.read_file('data/03_clustering/hexagons.csv', index_col=[0])\n",
    "    df_hex = df_hex[resolution].to_frame()\n",
    "    df_hex['geometry'] = df_hex.apply(lambda x: Polygon(h3.h3_to_geo_boundary(x[resolution], geo_json=True)), axis=1)\n",
    "    df_hex = df_hex.set_geometry(\"geometry\")\n",
    "    df_hex.crs = \"EPSG:4326\"\n",
    "    df_hex = df_hex.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # load point of interest\n",
    "    poi = gpd.read_file('data/03_clustering/POI.geojson')\n",
    "    \n",
    "    # join\n",
    "    poi_by_hex = gpd.sjoin(poi, df_hex, how='inner', predicate='within')\n",
    "    \n",
    "    return poi_by_hex.groupby(resolution).size().to_frame().reset_index().rename(columns={0:\"n_poi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_poi_for_resolution_and_amenity(resolution=\"h3_6\"):\n",
    "    # get polygons from hexagon id\n",
    "    df_hex = gpd.read_file('data/03_clustering/hexagons.csv', index_col=[0])\n",
    "    df_hex = df_hex[resolution].to_frame()\n",
    "    df_hex['geometry'] = df_hex.apply(lambda x: Polygon(h3.h3_to_geo_boundary(x[resolution], geo_json=True)), axis=1)\n",
    "    df_hex = df_hex.set_geometry(\"geometry\")\n",
    "    df_hex.crs = \"EPSG:4326\"\n",
    "    df_hex = df_hex.to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # load point of interest\n",
    "    poi = gpd.read_file('data/03_clustering/POI.geojson')\n",
    "    \n",
    "    # join\n",
    "    poi_by_hex = gpd.sjoin(poi, df_hex, how='inner', predicate='within').groupby([resolution, \"amenity\"]).size().to_frame().reset_index()\n",
    "    \n",
    "    # generate according dataframe with features\n",
    "    poi_features = []\n",
    "    for hexagon in poi_by_hex[resolution].unique():\n",
    "        di = {resolution: hexagon}\n",
    "        for amenity in poi_by_hex.amenity.unique():\n",
    "            n = poi_by_hex[(poi_by_hex[resolution] == hexagon) & (poi_by_hex[\"amenity\"] == amenity)][0].values\n",
    "            di[amenity] = n[0] if n.size > 0 else 0\n",
    "        poi_features.append(di)\n",
    "    \n",
    "    return pd.DataFrame(poi_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_by_hex = get_n_poi_for_resolution()\n",
    "poi_amenity_by_hex = get_n_poi_for_resolution_and_amenity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding POI data based on the hexagons \n",
    "taxi_df_clustering = taxi_df_clustering.merge(poi_by_hex[['h3_6','n_poi']], how=\"left\", on=\"h3_6\")\n",
    "taxi_df_clustering = taxi_df_clustering.merge(poi_amenity_by_hex[['h3_6','bar','pub','car_rental','clinic', 'ferry_terminal']], how=\"left\", on=\"h3_6\")\n",
    "taxi_df_clustering['bar/pub'] = taxi_df_clustering['bar']+taxi_df_clustering['pub']\n",
    "\n",
    "taxi_df_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to City center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As City Center, we will consider the central business district of the city, which is widely known as the Loop, the main section of Downtown Chicago (Wikipedia). This corresponds to the location of 41.881111 (Latitude), -87.629722 (Longtitude). We will investigate clustering trip types based on their pickup and dropoff distance to the city center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting both pickup and dropoff centroid location to a geoseries\n",
    "taxi_df_clustering['pickup_centroid_location'] = gpd.GeoSeries.from_wkt(taxi_df_clustering['pickup_centroid_location'])\n",
    "taxi_df_clustering['dropoff_centroid_location'] = gpd.GeoSeries.from_wkt(taxi_df_clustering['dropoff_centroid_location'])\n",
    "\n",
    "# Creating a GeoDataFrame \n",
    "taxi_geo_df_clustering = gpd.GeoDataFrame(taxi_df_clustering, geometry='pickup_centroid_location', crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting latitude and longitude of the pickup location\n",
    "taxi_df_clustering['pickup_centroid_location_lat'] = taxi_geo_df_clustering['pickup_centroid_location'].y\n",
    "taxi_df_clustering['pickup_centroid_location_lon'] = taxi_geo_df_clustering['pickup_centroid_location'].x\n",
    "\n",
    "# Extracting latitude and longitude of the dropoff location\n",
    "taxi_df_clustering['dropoff_centroid_location_lat'] = taxi_geo_df_clustering['dropoff_centroid_location'].y\n",
    "taxi_df_clustering['dropoff_centroid_location_lon'] = taxi_geo_df_clustering['dropoff_centroid_location'].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the distance between start coordinates and the city center\n",
    "for i in range(0, len(taxi_df_clustering)):\n",
    "    city_center = (41.881111, -87.629722)\n",
    "    start_coordinates = (taxi_df_clustering.at[i, 'pickup_centroid_location_lat'], taxi_df_clustering.at[i, 'pickup_centroid_location_lon'])\n",
    "    end_coordinates = (taxi_df_clustering.at[i, 'dropoff_centroid_location_lat'], taxi_df_clustering.at[i, 'dropoff_centroid_location_lon'])\n",
    "\n",
    "    # Units need to be set to \"Unit.MILES\"\n",
    "    taxi_df_clustering.at[i, 'distance_to_city_center_pickup'] = haversine(start_coordinates, city_center, unit=Unit.MILES)\n",
    "    taxi_df_clustering.at[i, 'distance_to_city_center_dropoff'] = haversine(end_coordinates, city_center, unit=Unit.MILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df_clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to use for Clustering\n",
    "Next we create a number of functions to help with frequent Clustering Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to standardize features\n",
    "def scalingData(dataframe):\n",
    "    newDataframe = dataframe.copy()\n",
    "    scaler = StandardScaler()\n",
    "    newDataframe[newDataframe.columns] = pd.DataFrame(scaler.fit_transform(newDataframe[newDataframe.columns]))\n",
    "    return newDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the Loss per cluster amount and plots the result of it in the range of 0-10 on the x-axis\n",
    "def calcAndPlotLossKM(clusterAmount, dataframe):\n",
    "    k_max = clusterAmount\n",
    "\n",
    "    clusters = []\n",
    "    losses = []\n",
    "\n",
    "    for k in range(k_max):\n",
    "        model = KMeans(n_clusters=k+1, n_init=10)\n",
    "        model.fit(dataframe)\n",
    "        clusters.append(k+1)\n",
    "        losses.append(model.inertia_)\n",
    "\n",
    "    fig = plt.subplots(figsize=(12,7))\n",
    "    plt.plot(clusters, losses)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.xlim([0,10])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates for a defined amount of clusters KMeans on the given dataframe\n",
    "def calcKMeans(numClusters, dataframe):\n",
    "    result = KMeans(n_clusters=numClusters, n_init=10)\n",
    "    result.fit(dataframe)\n",
    "\n",
    "    dataframe['ClusterKM'] = result.predict(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates for a defined amount of clusters GMM on the given dataframe\n",
    "def calcGMM(numClusters, dataframe):\n",
    "    gmm = GaussianMixture(n_components=numClusters).fit(dataframe)\n",
    "    dataframe['ClusterGMM'] = gmm.predict(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function describes every KMeans or GMM cluster with the describe() function for the original dataframe\n",
    "def describeData(originalDataframe, scaledDataframe, method):   \n",
    "    if(method == 'KMeans'):\n",
    "        for i in range(0, scaledDataframe['ClusterKM'].max()+1):\n",
    "            display(originalDataframe[scaledDataframe['ClusterKM'] == i].describe())\n",
    "    elif(method == 'GMM'):\n",
    "        for i in range(0, scaledDataframe['ClusterGMM'].max()+1):    \n",
    "            display(originalDataframe[scaledDataframe['ClusterGMM'] == i].describe())\n",
    "    else:\n",
    "        print('Error: The wrong method has been chosen. Either use \"KMeans\" or \"GMM\"!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function removes outliers for a set of columns, where values is n times larger than std\n",
    "def removeOutliers(df, columns, n_std):\n",
    "    for col in columns:\n",
    "        mean = df[col].mean()\n",
    "        sd = df[col].std()\n",
    "        \n",
    "        df = df[(df[col] <= mean + (n_std * sd))]\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Trip/Customer Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cluster Trip/Customer Types we will use the following features:\n",
    " - Miles of every trip\n",
    " - Distance to city center for both the pickup and dropoff locations\n",
    " - The coordinates of both the pickup and dropoff locations\n",
    " - The weekday of the trips\n",
    " - The start hour of the trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection by dropping unnecessary columns\n",
    "taxi_df_clustering.drop(columns=['trip_seconds',\n",
    "                                'trip_start_timestamp',\n",
    "                                'trip_end_timestamp',\n",
    "                                'pickup_census_tract',\n",
    "                                'dropoff_census_tract',\n",
    "                                'trip_total',\n",
    "                                'pickup_centroid_location',\n",
    "                                'dropoff_centroid_location',\n",
    "                                'idle_seconds',\n",
    "                                'end_hour',\n",
    "                                'month',\n",
    "                                'h3_6',\n",
    "                                'n_poi',\n",
    "                                'bar',\n",
    "                                'pub',\n",
    "                                'car_rental',\n",
    "                                'clinic',\n",
    "                                'ferry_terminal',\n",
    "                                'bar/pub'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting our features, we will have a look at our final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df_clustering.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a very high standard deviation for our \"trip_miles\", hence we will remove outliers 3 times bigger than standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers in dataset as impact on clustering is significant\n",
    "taxi_df_clustering = removeOutliers(taxi_df_clustering,['trip_miles'],3)\n",
    "\n",
    "taxi_df_clustering.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking care of outliers for \"trip_miles\" is necessary, since they can have a huge impact on the clustering performance. Now, we can scale our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "taxi_df_clustering_scaled = scalingData(taxi_df_clustering)\n",
    "taxi_df_clustering_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Since we have 9 features in our final dataframe, we use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pca = PCA().fit(taxi_df_clustering_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.plot(range(1,10), evaluate_pca.explained_variance_ratio_.cumsum(), marker ='o', linestyle='--')\n",
    "plt.title('Explained Variance by Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to keep the majority of variance explained by the components, we settle for 4 out of the 10 components which roughly preserve 80% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 4).fit(taxi_df_clustering_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pca = pca.transform(taxi_df_clustering_scaled)\n",
    "\n",
    "scores_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df_clustering_scaled_pca = pd.DataFrame(scores_pca, columns=['Component 1','Component 2','Component 3','Component 4'])\n",
    "\n",
    "taxi_df_clustering_scaled_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Clustering with Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with our soft clustering approach, we first calculate the loss with a hard clustering method (KMeans) to find out a possible number of clusters to pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the Loss with increasing number of Clusters\n",
    "calcAndPlotLossKM(10, taxi_df_clustering_scaled_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this we can try to pick cluster between 3 and 6 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft clustering calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy for KMeans\n",
    "taxi_df_clustering_gmm = taxi_df_clustering_scaled_pca.copy()\n",
    "\n",
    "# Calculating GMM\n",
    "calcGMM(3, taxi_df_clustering_gmm)\n",
    "\n",
    "# Adding the clusters from GMM into the original dataframe for visualization purposes\n",
    "taxi_df_clustering.loc[:, 'ClusterGMM'] = taxi_df_clustering_gmm['ClusterGMM']\n",
    "\n",
    "taxi_df_clustering_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describeData(taxi_df_clustering.loc[:, taxi_df_clustering.columns != 'ClusterGMM'], taxi_df_clustering_gmm, 'GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Clustering in 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Colors to map to clusters\n",
    "colors = {0:'#377eb8', 1:'#ff7f00', 2:'#4daf4a'}\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(xs=taxi_df_clustering_gmm['Component 1'], ys=taxi_df_clustering_gmm['Component 2'], zs=taxi_df_clustering_gmm['Component 3'], c=taxi_df_clustering_gmm['ClusterGMM'].map(colors))\n",
    "\n",
    "handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=v, label=k, markersize=8) for k, v in colors.items()]\n",
    "ax.legend(title='Cluster', handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title('Clustering Results GMM')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "ax.zaxis.labelpad = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Mapbox to display the clusters for the pickups in a map\n",
    "fig_pickup_gmm = px.scatter_mapbox(taxi_df_clustering, \n",
    "                        lat=\"pickup_centroid_location_lat\", lon=\"pickup_centroid_location_lon\", \n",
    "                        animation_frame='ClusterGMM', opacity=1, \n",
    "                        #animation_group='start_station_name', \n",
    "                        zoom=9.7, height=1000, width=900, \n",
    "                        title='Locational Clusters based on the pickup locations (GMM)')\n",
    "fig_pickup_gmm.update_geos(center=dict(lon=-87.629722, lat=41.881111))\n",
    "fig_pickup_gmm.update_layout(mapbox_style=\"carto-positron\")\n",
    "fig_pickup_gmm.update_layout(\n",
    "    title={\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "   font=dict(\n",
    "            size=18\n",
    "        ))\n",
    "display(fig_pickup_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Mapbox to display the clusters for the dropoffs in a map\n",
    "fig_dropoff_gmm = px.scatter_mapbox(taxi_df_clustering, \n",
    "                        lat=\"dropoff_centroid_location_lat\", lon=\"dropoff_centroid_location_lon\", \n",
    "                        animation_frame='ClusterGMM', opacity=1, \n",
    "                        #animation_group='start_station_name', \n",
    "                        zoom=9.7, height=1000, width=900, \n",
    "                        title='Locational Clusters based on the dropoff locations (GMM)')\n",
    "fig_dropoff_gmm.update_geos(center=dict(lon=-87.629722, lat=41.881111))\n",
    "fig_dropoff_gmm.update_layout(mapbox_style=\"carto-positron\")\n",
    "fig_dropoff_gmm.update_layout(\n",
    "    title={\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "   font=dict(\n",
    "            size=18\n",
    "        ))\n",
    "display(fig_dropoff_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Clustering with KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already calculated the loss for the initialization of our GMM clusters, so we do not need to do it here again. Same thing applies here as well, being 3 to 6 clusters as a choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy for KMeans\n",
    "taxi_df_clustering_kmeans = taxi_df_clustering_scaled_pca.copy()\n",
    "\n",
    "# Calculating KMeans\n",
    "calcKMeans(3, taxi_df_clustering_kmeans)\n",
    "\n",
    "# Adding the clusters from KMeans into the original dataframe for visualization purposes\n",
    "taxi_df_clustering.loc[:, 'ClusterKM'] = taxi_df_clustering_kmeans['ClusterKM']\n",
    "\n",
    "taxi_df_clustering_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Clustering in 3D\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Colors to map to clusters\n",
    "colors = {0:'#377eb8', 1:'#ff7f00', 2:'#4daf4a'}\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(xs=taxi_df_clustering_kmeans['Component 1'], ys=taxi_df_clustering_kmeans['Component 2'], zs=taxi_df_clustering_kmeans['Component 3'], c=taxi_df_clustering_kmeans['ClusterKM'].map(colors))\n",
    "\n",
    "handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=v, label=k, markersize=8) for k, v in colors.items()]\n",
    "ax.legend(title='Cluster', handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.title('Clustering Results KMeans')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "ax.set_zlabel('Component 3')\n",
    "ax.zaxis.labelpad = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describeData(taxi_df_clustering.loc[:, ~taxi_df_clustering.columns.isin(['ClusterGMM', 'ClusterKM'])], taxi_df_clustering_kmeans, 'KMeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Mapbox to display the clusters for the pickups (KMeans) in a map\n",
    "fig_pickup_kmeans = px.scatter_mapbox(taxi_df_clustering, \n",
    "                        lat=\"pickup_centroid_location_lat\", lon=\"pickup_centroid_location_lon\", \n",
    "                        animation_frame='ClusterKM', opacity=1, \n",
    "                        #animation_group='start_station_name', \n",
    "                        zoom=9.7, height=1000, width=900, \n",
    "                        title='Locational Clusters based on the pickup locations (KMeans)')\n",
    "fig_pickup_kmeans.update_geos(center=dict(lon=-87.629722, lat=41.881111))\n",
    "fig_pickup_kmeans.update_layout(mapbox_style=\"carto-positron\")\n",
    "fig_pickup_kmeans.update_layout(\n",
    "    title={\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "   font=dict(\n",
    "            size=18\n",
    "        ))\n",
    "display(fig_pickup_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Mapbox to display the clusters for the dropoffs (KMeans) in a map\n",
    "fig_pickup_kmeans = px.scatter_mapbox(taxi_df_clustering, \n",
    "                        lat=\"dropoff_centroid_location_lat\", lon=\"dropoff_centroid_location_lon\", \n",
    "                        animation_frame='ClusterKM', opacity=1, \n",
    "                        #animation_group='start_station_name', \n",
    "                        zoom=9.7, height=1000, width=900, \n",
    "                        title='Locational Clusters based on the dropoff locations (KMeans)')\n",
    "fig_pickup_kmeans.update_geos(center=dict(lon=-87.629722, lat=41.881111))\n",
    "fig_pickup_kmeans.update_layout(mapbox_style=\"carto-positron\")\n",
    "fig_pickup_kmeans.update_layout(\n",
    "    title={\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "   font=dict(\n",
    "            size=18\n",
    "        ))\n",
    "display(fig_pickup_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find spatial hot spots for trip demand we use Kernel density estimation with a gaussian kernel. The bandwidth selection is done by a \"rule of thumb\", called the ScottÂ´s Rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "taxi_df_locations = pd.read_parquet('data/01_prep/prepared/taxi_data_prepared.gzip')\n",
    "boroughs = gpd.read_file('data/03_clustering/Boundaries - City.geojson')\n",
    "\n",
    "# Sampling the data, because otherwise it takes way too long to compute or the kernel crashes\n",
    "taxi_df_locations = taxi_df_locations.sample(100000, random_state=0)\n",
    "taxi_df_locations.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both centroid locations to a geoseries\n",
    "taxi_df_locations['pickup_centroid_location'] = gpd.GeoSeries.from_wkt(taxi_df_locations['pickup_centroid_location'])\n",
    "taxi_df_locations['dropoff_centroid_location'] = gpd.GeoSeries.from_wkt(taxi_df_locations['dropoff_centroid_location'])\n",
    "\n",
    "# Creating GeoDataFrame\n",
    "taxi_geo_df_locations = gpd.GeoDataFrame(taxi_df_locations, geometry='pickup_centroid_location', crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levels correspond to iso-proportions of the density: e.g., 20% of the probability mass will lie below the contour drawn for 0.2.\n",
    "\n",
    "# KDE plot without clipping the data points\n",
    "levels = [0.2,0.4,0.6,0.8,1]\n",
    "\n",
    "kde = gplt.kdeplot(\n",
    "    taxi_geo_df_locations['pickup_centroid_location'], \n",
    "    figsize=(10,10),\n",
    "    levels = levels,\n",
    "    fill = True,\n",
    "    cmap = 'viridis',\n",
    "    alpha = 0.9,\n",
    "    legend = True,\n",
    "    projection=gplt.crs.WebMercator()\n",
    ")\n",
    "\n",
    "cx.add_basemap(kde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot with clipping the data points\n",
    "levels = [0.2,0.4,0.6,0.8,1]\n",
    "\n",
    "kde = gplt.kdeplot(\n",
    "    taxi_geo_df_locations['pickup_centroid_location'], \n",
    "    figsize=(10,10),\n",
    "    levels = levels,\n",
    "    fill = True,\n",
    "    cmap = 'viridis',\n",
    "    alpha = 0.9,\n",
    "    projection=gplt.crs.WebMercator(),\n",
    "    clip=boroughs.geometry\n",
    ")\n",
    "\n",
    "cx.add_basemap(kde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/from-kernel-density-estimation-to-spatial-analysis-in-python-64ddcdb6bc9b\n",
    "\n",
    "# Extracting the calculated polygons for better visualization purposes\n",
    "level_polygons = []\n",
    "i = 0\n",
    "for col in kde.collections:\n",
    "    paths = []\n",
    "    # Loop through all polygons that have the same intensity level\n",
    "    for contour in col.get_paths(): \n",
    "        # Create a polygon for the countour\n",
    "        # First polygon is the main countour, the rest are holes\n",
    "        for ncp,cp in enumerate(contour.to_polygons()):\n",
    "            x = cp[:,0]\n",
    "            y = cp[:,1]\n",
    "            new_shape = Polygon([(i[0], i[1]) for i in zip(x,y)])\n",
    "            if ncp == 0:\n",
    "                poly = new_shape\n",
    "            else:\n",
    "                # Remove holes, if any\n",
    "                poly = poly.difference(new_shape)\n",
    "\n",
    "        # Append polygon to list\n",
    "        paths.append(poly)\n",
    "    # Create a MultiPolygon for the contour\n",
    "    multi = MultiPolygon(paths)\n",
    "    # Append MultiPolygon and level as tuple to list\n",
    "    level_polygons.append((levels[i], multi))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the extraced multipolygons\n",
    "kde_df = pd.DataFrame(level_polygons, columns=['level', 'geometry'])\n",
    "\n",
    "# Convert the dataframe to a geodataframe\n",
    "kde_geo = gpd.GeoDataFrame(kde_df, geometry='geometry', crs=4326)\n",
    "\n",
    "# Change the crs for geometric operations\n",
    "kde_geo = kde_geo.to_crs(epsg=3035)\n",
    "\n",
    "# Calculate the area of every multipolygon\n",
    "kde_geo['area'] = kde_geo['geometry'].area\n",
    "\n",
    "# Change the crs back for visualization\n",
    "kde_geo = kde_geo.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive map representation of the estimated densities\n",
    "kde_geo.explore(column='level', cmap='viridis', legend=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
